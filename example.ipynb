{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7cab280f86b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "\n",
    "from hadl import HADL\n",
    "\n",
    "from darts.datasets import ETTh1Dataset\n",
    "\n",
    "torch.manual_seed(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Dataset Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Dataset and Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    CustomDataset is a PyTorch dataset class designed for handling time-series data, providing support for different types of forecasting tasks:\n",
    "    Univariate, Multivariate, and Multivariate-to-Univariate. This class allows for easy integration with PyTorch's DataLoader to generate\n",
    "    batches of time-series sequences and corresponding prediction targets.\n",
    "\n",
    "    Key Features:\n",
    "    1. **Univariate Forecasting (type=\"S\")**:\n",
    "      - The input (`data_x`) and the target (`data_y`) are the same, predicting a single time series based on past values of that same series.\n",
    "\n",
    "    2. **Multivariate Forecasting (type=\"M\")**:\n",
    "      - The input and target consist of all columns (features) of the time series. The task is to predict multiple series simultaneously.\n",
    "\n",
    "    3. **Multivariate-to-Univariate Forecasting (type=\"MS\")**:\n",
    "      - The input (`data_x`) consists of all columns except the last, and the target (`data_y`) is the last column. This is for scenarios where\n",
    "        multiple time series (features) are used to predict a single target series (last column).\n",
    "\n",
    "    The dataset class handles sequence generation by segmenting the input data into overlapping subsequences for training, validation, and testing.\n",
    "    It supports customization of sequence length (number of past time steps) and prediction length (future time steps to forecast).\n",
    "\n",
    "    Parameters:\n",
    "    - `data`: Input time-series data in the form of a NumPy array or pandas DataFrame.\n",
    "    - `seq_len`: Length of the historical time-series window used as input for each sample.\n",
    "    - `pred_len`: Length of the future time-series window used as the target for each sample.\n",
    "    - `kind`: Type of forecasting task: \"S\" for Univariate, \"M\" for Multivariate, \"MS\" for Multivariate-to-Univariate.\n",
    "    - `overlap`: Optional. Step size to create overlapping sequences. Default is 1, but can be set to higher values to create larger overlaps.\n",
    "\n",
    "    This dataset class works seamlessly with PyTorch's DataLoader to generate mini-batches of time-series sequences for model training, evaluation, and testing.\n",
    "\n",
    "    Note: The data must have target as last column.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, seq_len, pred_len, kind, overlap=1):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.overlap = overlap\n",
    "\n",
    "        match kind:\n",
    "            case \"S\":  # Univariate\n",
    "                self.data_x = self.data[:,-1].reshape(-1,1)  # Input data (last column)\n",
    "                self.data_y = self.data[:,-1].reshape(-1,1)  # Target data (last column)\n",
    "\n",
    "            case \"M\":  # Multivariate\n",
    "                self.data_x = self.data  # Input data (all columns)\n",
    "                self.data_y = self.data  # Target data (all columns)\n",
    "\n",
    "            # case \"MS\":  # Multivariate to Univariate\n",
    "            #     self.data_x = self.data[:, :-1]  # Input data (all except last column)\n",
    "            #     self.data_y = self.data[:, -1].reshape(-1,1)  # Target data (only last column)\n",
    "\n",
    "            case _:\n",
    "                self.data_x = self.data[:,-1].reshape(-1,1)  # Input data (last column)\n",
    "                self.data_y = self.data[:,-1].reshape(-1,1)  # Target data (last column)\n",
    "\n",
    "    def __len__(self):\n",
    "      return (len(self.data) - self.seq_len - self.pred_len) // self.overlap + 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      seq_start_idx = idx * self.overlap\n",
    "      seq_end_idx = seq_start_idx + self.seq_len\n",
    "\n",
    "      pred_start_idx = seq_end_idx\n",
    "      pred_end_idx = pred_start_idx + self.pred_len\n",
    "\n",
    "      # If the prediction end index goes out of bounds, make it equal to len(data) and other indexes to be modified accordinly.\n",
    "      if pred_end_idx > len(self.data):\n",
    "        pred_end_idx = len(self.data)\n",
    "        pred_start_idx = max(pred_end_idx - self.pred_len, 0)\n",
    "        seq_end_idx = max(pred_end_idx - self.pred_len, self.seq_len)\n",
    "        seq_start_idx = max(seq_end_idx - self.seq_len, 0)\n",
    "\n",
    "\n",
    "\n",
    "      seq_x = self.data_x[seq_start_idx:seq_end_idx, :]\n",
    "      seq_y = self.data_y[pred_start_idx:pred_end_idx, :]\n",
    "\n",
    "      return seq_x, seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loader(time_series, batch_size=32, seq_len=96, pred_len=6, kind=\"S\", overlap=4, split=(0.7,0.1,0.2), sc=StandardScaler()):\n",
    "  \"\"\"\n",
    "    Loader function to create DataLoader objects for training, validation, and testing of time-series data.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: NumPy array or pandas DataFrame containing the time-series data.\n",
    "    - batch_size: Number of samples per batch of data. Default is 32.\n",
    "    - seq_len: Length of the input sequence (number of time steps for past data). Default is 96.\n",
    "    - pred_len: Length of the prediction sequence (number of time steps to forecast). Default is 6.\n",
    "    - kind: Type of forecasting task. \"S\" for univariate, \"M\" for multivariate, \"MS\" for multivariate-to-univariate. Default is \"S\".\n",
    "    - overlap: Step size for overlapping sequences (affects how sequences are created). Default is 4.\n",
    "    - split: Tuple representing the ratio of the dataset to be split into training, validation, and testing sets. Default is (0.7, 0.1, 0.2).\n",
    "\n",
    "    Returns:\n",
    "    - train_loader: DataLoader object for the training dataset.\n",
    "    - val_loader: DataLoader object for the validation dataset.\n",
    "    - test_loader: DataLoader object for the testing dataset.\n",
    "\n",
    "    This function standardizes the data, splits it into train, validation, and test sets, and creates DataLoader objects\n",
    "    with the specified batch size, sequence length, prediction length, and overlap for each set.\n",
    "\n",
    "    Note:\n",
    "    Shuffle is set to \"False\", since it is time series.\n",
    "    Drop_last is set to \"True\", since while create sequences, last batch may not be same size as others.\n",
    "    Time Series's target must be last column.\n",
    "    \"\"\"\n",
    "\n",
    "  if isinstance(time_series, pd.DataFrame):\n",
    "    time_series = time_series.to_numpy()\n",
    "\n",
    "  ## Borders for Train=70%, Validation=10% and Test=20%\n",
    "  borders =(int(time_series.shape[0] * split[0]), int(time_series.shape[0] * (split[0] + split[1])), int(time_series.shape[0]))\n",
    "\n",
    "  train_data = time_series[:borders[0],:]\n",
    "  val_data = time_series[borders[0]:borders[1],:]\n",
    "  test_data = time_series[borders[1]:borders[2],:]\n",
    "\n",
    "  ## Perform standardization of the dataset.\n",
    "  sc.fit(train_data)\n",
    "  train_data = sc.transform(train_data)\n",
    "  val_data = sc.transform(val_data)\n",
    "  test_data = sc.transform(test_data)\n",
    "\n",
    "  ##\n",
    "  train_data = torch.from_numpy(train_data).float()\n",
    "  val_data = torch.from_numpy(val_data).float()\n",
    "  test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "  ## Create train, val and test loader.\n",
    "  train_dataset = CustomDataset(train_data, seq_len=seq_len, pred_len=pred_len, kind=kind, overlap=overlap)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "  val_dataset = CustomDataset(val_data, seq_len=seq_len, pred_len=pred_len, kind=kind, overlap=overlap)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "  test_dataset = CustomDataset(test_data, seq_len=seq_len, pred_len=pred_len, kind=kind, overlap=overlap)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "  return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs=30, patience=10, lr=0.01):\n",
    "    # Device setup (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for seq_x, seq_y in train_loader:\n",
    "            seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(seq_x)\n",
    "            loss = criterion(outputs, seq_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        # Average Train Loss\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seq_x, seq_y in val_loader:\n",
    "                seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "                outputs = model(seq_x)\n",
    "                loss = criterion(outputs, seq_y)\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Reduce Learning Rate if Validation Loss Plateaus\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            early_stopping_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save best model state\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "        # Print Training Progress\n",
    "        if epoch % 3 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def MAE(pred, true):\n",
    "        return np.mean(np.abs(pred - true))\n",
    "\n",
    "    def MSE(pred, true):\n",
    "        return np.mean((pred - true) ** 2)\n",
    "\n",
    "    def RMSE(pred, true):\n",
    "        return np.sqrt(MSE(pred, true))\n",
    "\n",
    "    test_predictions = []\n",
    "    test_actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_y in test_loader:\n",
    "            seq_x, seq_y = seq_x.to(device), seq_y.to(device)\n",
    "            outputs = model(seq_x)\n",
    "            test_predictions.append(outputs.cpu().numpy())  # Convert tensor to numpy\n",
    "            test_actuals.append(seq_y.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "    test_actuals = np.concatenate(test_actuals, axis=0)\n",
    "\n",
    "    # Calculate Metrics\n",
    "    mae = MAE(test_actuals, test_predictions)\n",
    "    mse = MSE(test_actuals, test_predictions)\n",
    "    rmse = RMSE(test_actuals, test_predictions)\n",
    "\n",
    "    print(f'Test MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
    "    return mae, mse, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(model):\n",
    "  # Calculate the number of trainable and non-trainable parameters\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  non_trainable_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "\n",
    "  print(f\"Non_trainable: {non_trainable_params} Trainable:{trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17420, 7)\n"
     ]
    }
   ],
   "source": [
    "time_series = ETTh1Dataset().load().values()\n",
    "print(time_series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-val-test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512 #Input Sequence Length\n",
    "pred_len = 96 # Prediction Length\n",
    "overlap = 4 # overlapping between each input length to create batches.\n",
    "kind = \"M\" # M=Multivariate, S=univariate\n",
    "batch_size=32 #batch size\n",
    "split= (0.7,0.1,0.2) # train,val,,test split ratio.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_loader, val_loader, test_loader = Loader(time_series,\n",
    "                                               batch_size=batch_size,\n",
    "                                               seq_len=seq_len,\n",
    "                                               pred_len=pred_len,\n",
    "                                               kind=kind,\n",
    "                                               overlap=overlap,\n",
    "                                               split=split,\n",
    "                                               sc=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.6753, Val Loss: 0.3972, LR: 0.010000\n",
      "Epoch [4/30], Train Loss: 0.3927, Val Loss: 0.3579, LR: 0.010000\n",
      "Epoch [7/30], Train Loss: 0.3910, Val Loss: 0.3591, LR: 0.010000\n",
      "Epoch [10/30], Train Loss: 0.3907, Val Loss: 0.3596, LR: 0.001000\n",
      "Epoch [13/30], Train Loss: 0.3821, Val Loss: 0.3593, LR: 0.001000\n",
      "Epoch [16/30], Train Loss: 0.3833, Val Loss: 0.3594, LR: 0.001000\n",
      "Epoch [19/30], Train Loss: 0.3837, Val Loss: 0.3595, LR: 0.001000\n",
      "Epoch [22/30], Train Loss: 0.3827, Val Loss: 0.3596, LR: 0.000100\n",
      "Epoch [25/30], Train Loss: 0.3828, Val Loss: 0.3596, LR: 0.000100\n",
      "Epoch [28/30], Train Loss: 0.3828, Val Loss: 0.3596, LR: 0.000100\n",
      "Test MAE: 0.4827, MSE: 0.4573, RMSE: 0.6762\n",
      "Non_trainable: 0 Trainable:10656\n"
     ]
    }
   ],
   "source": [
    "## Default settings\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=True, enable_DCT=True)\n",
    "mod_model1 = train(model, train_loader, val_loader, num_epochs=30, patience=30, lr=0.01)\n",
    "test(mod_model1, test_loader)\n",
    "calc_params(mod_model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.7862, Val Loss: 0.4485, LR: 0.010000\n",
      "Epoch [4/30], Train Loss: 0.5280, Val Loss: 0.4361, LR: 0.010000\n",
      "Epoch [7/30], Train Loss: 0.5057, Val Loss: 0.4372, LR: 0.010000\n",
      "Epoch [10/30], Train Loss: 0.4990, Val Loss: 0.4376, LR: 0.001000\n",
      "Epoch [13/30], Train Loss: 0.4786, Val Loss: 0.4372, LR: 0.001000\n",
      "Epoch [16/30], Train Loss: 0.4804, Val Loss: 0.4376, LR: 0.001000\n",
      "Epoch [19/30], Train Loss: 0.4811, Val Loss: 0.4377, LR: 0.001000\n",
      "Epoch [22/30], Train Loss: 0.4787, Val Loss: 0.4379, LR: 0.000100\n",
      "Epoch [25/30], Train Loss: 0.4790, Val Loss: 0.4380, LR: 0.000100\n",
      "Epoch [28/30], Train Loss: 0.4792, Val Loss: 0.4381, LR: 0.000100\n",
      "Test MAE: 0.5845, MSE: 0.6155, RMSE: 0.7845\n",
      "Non_trainable: 0 Trainable:74592\n"
     ]
    }
   ],
   "source": [
    "## Enable Individual\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=True, enable_Haar=True, enable_DCT=True)\n",
    "mod_model2 = train(model, train_loader, val_loader, num_epochs=30, patience=30, lr=0.01)\n",
    "test(mod_model2, test_loader)\n",
    "calc_params(mod_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 0.7577, Val Loss: 0.4153, LR: 0.010000\n",
      "Epoch [4/30], Train Loss: 0.4281, Val Loss: 0.3713, LR: 0.010000\n",
      "Epoch [7/30], Train Loss: 0.4140, Val Loss: 0.3656, LR: 0.010000\n",
      "Epoch [10/30], Train Loss: 0.4120, Val Loss: 0.3659, LR: 0.001000\n",
      "Epoch [13/30], Train Loss: 0.4012, Val Loss: 0.3676, LR: 0.001000\n",
      "Epoch [16/30], Train Loss: 0.4033, Val Loss: 0.3683, LR: 0.001000\n",
      "Epoch [19/30], Train Loss: 0.4041, Val Loss: 0.3688, LR: 0.001000\n",
      "Epoch [22/30], Train Loss: 0.4029, Val Loss: 0.3689, LR: 0.000100\n",
      "Epoch [25/30], Train Loss: 0.4031, Val Loss: 0.3690, LR: 0.000100\n",
      "Epoch [28/30], Train Loss: 0.4032, Val Loss: 0.3691, LR: 0.000100\n",
      "Test MAE: 0.4966, MSE: 0.4802, RMSE: 0.6930\n",
      "Non_trainable: 0 Trainable:18336\n"
     ]
    }
   ],
   "source": [
    "## Disable Haar\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=False, enable_DCT=True)\n",
    "mod_model3 = train(model, train_loader, val_loader, num_epochs=30, patience=30, lr=0.01)\n",
    "test(mod_model3, test_loader)\n",
    "calc_params(mod_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 1.2558, Val Loss: 0.4411, LR: 0.010000\n",
      "Epoch [4/30], Train Loss: 0.4409, Val Loss: 0.3941, LR: 0.010000\n",
      "Epoch [7/30], Train Loss: 0.4180, Val Loss: 0.3691, LR: 0.010000\n",
      "Epoch [10/30], Train Loss: 0.4725, Val Loss: 0.3989, LR: 0.001000\n",
      "Epoch [13/30], Train Loss: 0.3739, Val Loss: 0.3666, LR: 0.001000\n",
      "Epoch [16/30], Train Loss: 0.3722, Val Loss: 0.3705, LR: 0.001000\n",
      "Epoch [19/30], Train Loss: 0.3734, Val Loss: 0.3720, LR: 0.001000\n",
      "Epoch [22/30], Train Loss: 0.3654, Val Loss: 0.3637, LR: 0.000100\n",
      "Epoch [25/30], Train Loss: 0.3626, Val Loss: 0.3615, LR: 0.000100\n",
      "Epoch [28/30], Train Loss: 0.3624, Val Loss: 0.3611, LR: 0.000100\n",
      "Test MAE: 0.4689, MSE: 0.4482, RMSE: 0.6695\n",
      "Non_trainable: 0 Trainable:10656\n"
     ]
    }
   ],
   "source": [
    "## Disable DCT\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=True, enable_DCT=False)\n",
    "mod_model4 = train(model, train_loader, val_loader, num_epochs=30, patience=30, lr=0.01)\n",
    "test(mod_model4, test_loader)\n",
    "calc_params(mod_model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 1.1898, Val Loss: 0.4308, LR: 0.010000\n",
      "Epoch [4/30], Train Loss: 0.4804, Val Loss: 0.4160, LR: 0.010000\n",
      "Epoch [7/30], Train Loss: 0.4411, Val Loss: 0.3965, LR: 0.010000\n",
      "Epoch [10/30], Train Loss: 0.4473, Val Loss: 0.3903, LR: 0.001000\n",
      "Epoch [13/30], Train Loss: 0.3711, Val Loss: 0.3597, LR: 0.001000\n",
      "Epoch [16/30], Train Loss: 0.3679, Val Loss: 0.3598, LR: 0.001000\n",
      "Epoch [19/30], Train Loss: 0.3667, Val Loss: 0.3597, LR: 0.001000\n",
      "Epoch [22/30], Train Loss: 0.3608, Val Loss: 0.3581, LR: 0.000100\n",
      "Epoch [25/30], Train Loss: 0.3601, Val Loss: 0.3573, LR: 0.000100\n",
      "Epoch [28/30], Train Loss: 0.3600, Val Loss: 0.3570, LR: 0.000100\n",
      "Test MAE: 0.4713, MSE: 0.4514, RMSE: 0.6718\n",
      "Non_trainable: 0 Trainable:18336\n"
     ]
    }
   ],
   "source": [
    "## Disable Haar and DCT\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=False, enable_DCT=False)\n",
    "mod_model5 = train(model, train_loader, val_loader, num_epochs=30, patience=30, lr=0.01)\n",
    "test(mod_model5, test_loader)\n",
    "calc_params(mod_model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
