{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler, StandardScaler\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader, default_collate\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhadl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HADL\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ETTh1Dataset\n\u001b[1;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m2003\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "\n",
    "from .hadl import HADL\n",
    "\n",
    "from darts.datasets import ETTh1Dataset\n",
    "\n",
    "torch.manual_seed(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Dataset Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Dataset and Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    CustomDataset is a PyTorch dataset class designed for handling time-series data, providing support for different types of forecasting tasks:\n",
    "    Univariate, Multivariate, and Multivariate-to-Univariate. This class allows for easy integration with PyTorch's DataLoader to generate\n",
    "    batches of time-series sequences and corresponding prediction targets.\n",
    "\n",
    "    Key Features:\n",
    "    1. **Univariate Forecasting (type=\"S\")**:\n",
    "      - The input (`data_x`) and the target (`data_y`) are the same, predicting a single time series based on past values of that same series.\n",
    "\n",
    "    2. **Multivariate Forecasting (type=\"M\")**:\n",
    "      - The input and target consist of all columns (features) of the time series. The task is to predict multiple series simultaneously.\n",
    "\n",
    "    3. **Multivariate-to-Univariate Forecasting (type=\"MS\")**:\n",
    "      - The input (`data_x`) consists of all columns except the last, and the target (`data_y`) is the last column. This is for scenarios where\n",
    "        multiple time series (features) are used to predict a single target series (last column).\n",
    "\n",
    "    The dataset class handles sequence generation by segmenting the input data into overlapping subsequences for training, validation, and testing.\n",
    "    It supports customization of sequence length (number of past time steps) and prediction length (future time steps to forecast).\n",
    "\n",
    "    Parameters:\n",
    "    - `data`: Input time-series data in the form of a NumPy array or pandas DataFrame.\n",
    "    - `seq_len`: Length of the historical time-series window used as input for each sample.\n",
    "    - `pred_len`: Length of the future time-series window used as the target for each sample.\n",
    "    - `kind`: Type of forecasting task: \"S\" for Univariate, \"M\" for Multivariate, \"MS\" for Multivariate-to-Univariate.\n",
    "    - `overlap`: Optional. Step size to create overlapping sequences. Default is 1, but can be set to higher values to create larger overlaps.\n",
    "\n",
    "    This dataset class works seamlessly with PyTorch's DataLoader to generate mini-batches of time-series sequences for model training, evaluation, and testing.\n",
    "\n",
    "    Note: The data must have target as last column.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, seq_len, pred_len, kind, overlap=1):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.overlap = overlap\n",
    "\n",
    "        match kind:\n",
    "            case \"S\":  # Univariate\n",
    "                self.data_x = self.data[:,-1].reshape(-1,1)  # Input data (last column)\n",
    "                self.data_y = self.data[:,-1].reshape(-1,1)  # Target data (last column)\n",
    "\n",
    "            case \"M\":  # Multivariate\n",
    "                self.data_x = self.data  # Input data (all columns)\n",
    "                self.data_y = self.data  # Target data (all columns)\n",
    "\n",
    "            case \"MS\":  # Multivariate to Univariate\n",
    "                self.data_x = self.data[:, :-1]  # Input data (all except last column)\n",
    "                self.data_y = self.data[:, -1].reshape(-1,1)  # Target data (only last column)\n",
    "\n",
    "            case _:\n",
    "                self.data_x = self.data[:,-1].reshape(-1,1)  # Input data (last column)\n",
    "                self.data_y = self.data[:,-1].reshape(-1,1)  # Target data (last column)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      seq_start_idx = idx * self.overlap\n",
    "      seq_end_idx = seq_start_idx + self.seq_len\n",
    "\n",
    "      pred_start_idx = seq_end_idx\n",
    "      pred_end_idx = pred_start_idx + self.pred_len\n",
    "\n",
    "      # If the prediction end index goes out of bounds, make it equal to len(data) and other indexes to be modified accordinly.\n",
    "      if pred_end_idx > len(self.data):\n",
    "         pred_end_idx = len(self.data)\n",
    "         pred_start_idx = pred_end_idx - self.pred_len\n",
    "\n",
    "         seq_end_idx = pred_end_idx\n",
    "         seq_start_idx = seq_end_idx - self.seq_len\n",
    "\n",
    "\n",
    "      seq_x = self.data_x[seq_start_idx:seq_end_idx, :]\n",
    "      seq_y = self.data_y[pred_start_idx:pred_end_idx, :]\n",
    "\n",
    "      return seq_x, seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loader(time_series, batch_size=32, seq_len=96, pred_len=6, kind=\"S\", overlap=4, split=(0.7,0.1,0.2), sc=StandardScaler()):\n",
    "  \"\"\"\n",
    "    Loader function to create DataLoader objects for training, validation, and testing of time-series data.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series: NumPy array or pandas DataFrame containing the time-series data.\n",
    "    - batch_size: Number of samples per batch of data. Default is 32.\n",
    "    - seq_len: Length of the input sequence (number of time steps for past data). Default is 96.\n",
    "    - pred_len: Length of the prediction sequence (number of time steps to forecast). Default is 6.\n",
    "    - kind: Type of forecasting task. \"S\" for univariate, \"M\" for multivariate, \"MS\" for multivariate-to-univariate. Default is \"S\".\n",
    "    - overlap: Step size for overlapping sequences (affects how sequences are created). Default is 4.\n",
    "    - split: Tuple representing the ratio of the dataset to be split into training, validation, and testing sets. Default is (0.7, 0.1, 0.2).\n",
    "\n",
    "    Returns:\n",
    "    - train_loader: DataLoader object for the training dataset.\n",
    "    - val_loader: DataLoader object for the validation dataset.\n",
    "    - test_loader: DataLoader object for the testing dataset.\n",
    "\n",
    "    This function standardizes the data, splits it into train, validation, and test sets, and creates DataLoader objects\n",
    "    with the specified batch size, sequence length, prediction length, and overlap for each set.\n",
    "\n",
    "    Note:\n",
    "    Shuffle is set to \"False\", since it is time series.\n",
    "    Drop_last is set to \"True\", since while create sequences, last batch may not be same size as others.\n",
    "    Time Series's target must be last column.\n",
    "    \"\"\"\n",
    "\n",
    "  ## Borders for Train=70%, Validation=10% and Test=20%\n",
    "  borders =(int(time_series.shape[0] * split[0]), int(time_series.shape[0] * (split[0] + split[1])), int(time_series.shape[0]))\n",
    "\n",
    "  train_data = time_series[:borders[0],:]\n",
    "  val_data = time_series[borders[0]:borders[1],:]\n",
    "  test_data = time_series[borders[1]:borders[2],:]\n",
    "\n",
    "  ## Perform standardization of the dataset.\n",
    "  train_data = sc.fit_transform(train_data)\n",
    "  val_data = sc.transform(val_data)\n",
    "  test_data = sc.transform(test_data)\n",
    "\n",
    "  ##\n",
    "  train_data = torch.from_numpy(train_data).float()\n",
    "  val_data = torch.from_numpy(val_data).float()\n",
    "  test_data = torch.from_numpy(test_data).float()\n",
    "\n",
    "  ## Create train, val and test loader.\n",
    "  train_dataset = CustomDataset(train_data, seq_len=seq_len, pred_len=pred_len, kind=kind, overlap=overlap)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "  val_dataset = CustomDataset(val_data, seq_len=seq_len, pred_len=pred_len, kind=kind, overlap=overlap)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "  test_dataset = CustomDataset(test_data, seq_len=seq_len, pred_len=pred_len, kind=kind, overlap=overlap)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "  return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs=30, patience=5, lr=0.01):\n",
    "  # Training Loop\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  best_val_loss = float('inf')\n",
    "  early_stopping_counter = 0\n",
    "  num_epochs = num_epochs\n",
    "  patience = patience\n",
    "  lr = lr\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "      epoch_train_loss = 0.0\n",
    "      for seq_x, seq_y in train_loader:\n",
    "          #print(seq_x.shape)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(seq_x)\n",
    "          loss = criterion(outputs, seq_y)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          epoch_train_loss += loss.item()\n",
    "\n",
    "      # Calculate Average Train Loss\n",
    "      avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "      train_losses.append(avg_train_loss)\n",
    "\n",
    "      # Validation\n",
    "      model.eval()\n",
    "      epoch_val_loss = 0.0\n",
    "      with torch.no_grad():\n",
    "          for seq_x, seq_y in val_loader:\n",
    "              outputs = model(seq_x)\n",
    "              loss = criterion(outputs, seq_y)\n",
    "              epoch_val_loss += loss.item()\n",
    "\n",
    "      # Calculate Average Validation Loss\n",
    "      avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "      val_losses.append(avg_val_loss)\n",
    "\n",
    "      # Learning Rate Scheduling\n",
    "      scheduler.step(avg_val_loss)\n",
    "\n",
    "      # Early Stopping\n",
    "      if avg_val_loss < best_val_loss:\n",
    "          best_val_loss = avg_val_loss\n",
    "          early_stopping_counter = 0\n",
    "      else:\n",
    "          early_stopping_counter += 1\n",
    "          if early_stopping_counter >= patience:\n",
    "              print(f'Early stopping at epoch {epoch+1}')\n",
    "              break\n",
    "\n",
    "      if epoch % 3 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "\n",
    "  def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "\n",
    "  def MSE(pred, true):\n",
    "      return np.mean((pred - true) ** 2)\n",
    "\n",
    "\n",
    "  def RMSE(pred, true):\n",
    "      return np.sqrt(MSE(pred, true))\n",
    "\n",
    "\n",
    "  # Evaluation on Test Set\n",
    "  model.eval()\n",
    "  test_predictions = []\n",
    "  test_actuals = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for seq_x, seq_y in test_loader:\n",
    "          outputs = model(seq_x)\n",
    "          test_predictions.append(outputs.numpy())\n",
    "          test_actuals.append(seq_y.numpy())\n",
    "\n",
    "\n",
    "  test_predictions = np.concatenate(test_predictions)\n",
    "  test_actuals = np.concatenate(test_actuals)\n",
    "\n",
    "\n",
    "  print(test_predictions.shape, test_actuals.shape)\n",
    "\n",
    "  # Calculate Metrics\n",
    "  mae = MAE(test_actuals, test_predictions)\n",
    "  mse = MSE(test_actuals, test_predictions)\n",
    "  rmse = RMSE(test_actuals, test_predictions)\n",
    "\n",
    "  print(f'Test MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = ETTh1Dataset().load().values()\n",
    "print(time_series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train-val-test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 336 #Input Sequence Length\n",
    "pred_len = 90 # Prediction Length\n",
    "overlap = 4 # overlapping between each input length to create batches.\n",
    "kind = \"S\" # M=Multivariate, S=univariate\n",
    "batch_size=32 #batch size\n",
    "split= (0.7,0.1,0.2) # train,val,,test split ratio.\n",
    "\n",
    "train_loader, val_loader, test_loader = Loader(time_series,\n",
    "                                               batch_size=batch_size,\n",
    "                                               seq_len=seq_len,\n",
    "                                               pred_len=pred_len,\n",
    "                                               kind=kind,\n",
    "                                               overlap=overlap,\n",
    "                                               split=split,\n",
    "                                               sc=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default settings\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=True, enable_DCT=True)\n",
    "mod_model = train(model, train_loader, val_loader)\n",
    "test(mod_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable Individual\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=True, enable_Haar=True, enable_DCT=True)\n",
    "mod_model = train(model, train_loader, val_loader)\n",
    "test(mod_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disable Haar\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=False, enable_DCT=True)\n",
    "mod_model = train(model, train_loader, val_loader)\n",
    "test(mod_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disable DCT\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=True, enable_DCT=False)\n",
    "mod_model = train(model, train_loader, val_loader)\n",
    "test(mod_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disable Haar and DCT\n",
    "## Disable DCT\n",
    "model = HADL(seq_len=seq_len, pred_len=pred_len, features=7, rank=30, individual=False, enable_Haar=False, enable_DCT=False)\n",
    "mod_model = train(model, train_loader, val_loader)\n",
    "test(mod_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
